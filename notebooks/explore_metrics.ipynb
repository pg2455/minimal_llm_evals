{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244532f2",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "There are many metrics. `lighteval` is the library that has the implementations of the most commonly used metrics in NLP. \n",
    "It comes really handy to understand these metrics and how they are computed (source code: [https://github.com/huggingface/lighteval/tree/v0.11.0/src/lighteval/metrics](https://github.com/huggingface/lighteval/tree/v0.11.0/src/lighteval/metrics)) \n",
    "\n",
    "\n",
    "The webpage: [https://huggingface.co/docs/lighteval/en/package_reference/metrics#sample-metrics](https://huggingface.co/docs/lighteval/en/package_reference/metrics#sample-metrics) gives you the high-level API of different metrics.\n",
    "\n",
    "\n",
    "#### Normalizations\n",
    "There have been a lot of NLP papers that propose metrics and evalautions, they might end up using different preprocessing and metrics. \n",
    "The library brings all together the the page pointed to above host all those functions in `normalizations.py`. Example: Math based: `gsm8k_normalizer`, `math_normalizer` or language based normalizers such as `helm_noramlizer`. A look can teach you a lot about the preprocessing steps. \n",
    "\n",
    "\n",
    "### Directly running the evaluation using lighteval\n",
    "\n",
    "You can directly evaluate any LM using the lighteval-cli by providing the model and the task name\n",
    "\n",
    "\n",
    "```bash\n",
    "lighteval \\\n",
    "    --model_args pretrained=gpt2 \\\n",
    "    --tasks hellaswag \\\n",
    "    --batch_size 8 \\\n",
    "    --output_dir ./results\n",
    "```\n",
    "\n",
    "Also an option to use faster inference servers or multi-GPU\n",
    "```bash\n",
    "export VLLM_WORKER_MULTIPROC_METHOD=spawn\n",
    "lighteval vllm \\\n",
    "  \"model_name=meta-llama/Llama-3.1-8B-Instruct,tensor_parallel_size=2\" \\\n",
    "  \"lighteval|gsm8k|5\"\n",
    "```\n",
    "\n",
    "or an API\n",
    "\n",
    "```bash\n",
    "lighteval endpoint litellm \\\n",
    "  \"provider=openai,model_name=gpt-4o-mini\" \\\n",
    "  \"lighteval|mmlu|0\"\n",
    "```\n",
    "\n",
    "This is handy, but for now we focus on dealing with what goes on underneath it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d388d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lighteval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa56086",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Definitions\n",
    "\n",
    "* **Gold $g$**: the reference text provided by the dataset. For example, in QA, $g$ might be “Paris” if the question is “What is the capital of France?”. In summarization, $g$ is the human-written summary.\n",
    "\n",
    "* **Prediction $p$**: the model’s output. Depending on the setting this could be:\n",
    "\n",
    "  * a text string sampled from the model’s logits, e.g. “Paris, France”.\n",
    "  * a log-probability vector over choices, e.g. $(\\ell_1,\\dots,\\ell_m)$ if there are $m$ candidate answers.\n",
    "  * multiple samples $p^{(1)},p^{(2)},\\dots,p^{(n)}$ from the same prompt, for multi-sample metrics.\n",
    "\n",
    "* **Choice set $\\{c_1,\\dots,c_m\\}$**: in multiple-choice or classification tasks, the discrete set of possible answers.\n",
    "\n",
    "---\n",
    "\n",
    "# String-based metrics\n",
    "\n",
    "These compare prediction text $p$ to reference $g$ directly. They do not need access to the evaluated model’s logits. They may, however, use an **external pretrained LM** (e.g. BLEURT, BERTScore, SummaCZS).\n",
    "\n",
    "---\n",
    "\n",
    "### Exact Match\n",
    "\n",
    "Checks whether prediction exactly matches the gold (or its prefix/suffix).\n",
    "\n",
    "* **Input:** prediction string $p$, reference string(s) $g$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{EM}(p,g) = \n",
    "  \\begin{cases}\n",
    "  1 & \\nu(p)=\\nu(g) \\quad \\text{(full)}\\\\\n",
    "  1 & \\nu(p)\\text{ startswith }\\nu(g) \\quad \\text{(prefix)}\\\\\n",
    "  1 & \\nu(p)\\text{ endswith }\\nu(g) \\quad \\text{(suffix)}\\\\\n",
    "  0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "  where $\\nu(\\cdot)$ is optional normalization (strip, lowercase, etc.).\n",
    "* **When to use:** Useful in tasks with unambiguous answers (math word problems, factual QA).\n",
    "* **When not:** Not good for free-form answers with paraphrases.\n",
    "* **LLM use:** **No external LM**; purely heuristic.\n",
    "\n",
    "---\n",
    "\n",
    "### F1 over Bag-of-Words\n",
    "\n",
    "Measures token-level overlap between gold and prediction.\n",
    "\n",
    "* **Input:** sets of tokens $G=\\text{tok}(g)$, $P=\\text{tok}(p)$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{Prec} = \\frac{|P \\cap G|}{|P|},\\quad\n",
    "  \\text{Rec} = \\frac{|P \\cap G|}{|G|},\\quad\n",
    "  F1 = \\frac{2\\cdot\\text{Prec}\\cdot\\text{Rec}}{\\text{Prec}+\\text{Rec}}\n",
    "  $$\n",
    "* **When to use:** QA benchmarks like SQuAD, where partial overlap counts.\n",
    "* **When not:** Sensitive to synonyms; not good for paraphrase-heavy tasks.\n",
    "* **LLM use:** **No external LM**.\n",
    "\n",
    "---\n",
    "\n",
    "### ROUGE (1/2/L/Lsum)\n",
    "\n",
    "Overlap-based metrics, widely used for summarization.\n",
    "\n",
    "* **Input:** n-grams or longest common subsequence between $p$ and $g$.\n",
    "* **Formulation:**\n",
    "  For ROUGE-n:\n",
    "\n",
    "  $$\n",
    "  \\text{ROUGE-n} = \\frac{\\sum_{k\\in \\text{n-grams}} \\min(\\text{count}_p(k),\\ \\text{count}_g(k))}{\\sum_{k\\in \\text{n-grams}} \\text{count}_g(k)}\n",
    "  $$\n",
    "\n",
    "  Output is typically recall, precision, and F1.\n",
    "* **When to use:** Summarization tasks.\n",
    "* **When not:** Not sensitive to meaning; can reward copying.\n",
    "* **LLM use:** **No external LM**.\n",
    "\n",
    "---\n",
    "\n",
    "### BLEU\n",
    "\n",
    "n-gram precision metric with brevity penalty.\n",
    "\n",
    "* **Input:** n-grams from prediction and references.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{BLEU} = \\text{BP}\\cdot\\exp\\left(\\sum_{n=1}^N w_n \\log p_n\\right)\n",
    "  $$\n",
    "\n",
    "  where $p_n$ is the modified n-gram precision and $\\text{BP}$ is brevity penalty.\n",
    "* **When to use:** Machine translation.\n",
    "* **When not:** Sentence-level BLEU is unstable; not good for short answers.\n",
    "* **LLM use:** **No external LM**.\n",
    "\n",
    "---\n",
    "\n",
    "### BLEURT\n",
    "\n",
    "Learned metric trained to align with human judgments.\n",
    "\n",
    "* **Input:** gold $g$, pred $p$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{BLEURT}(p,g) = f_\\theta(p,g)\n",
    "  $$\n",
    "\n",
    "  where $f_\\theta$ is a pretrained BERT-like regression model.\n",
    "* **When to use:** Text generation tasks where semantic similarity matters.\n",
    "* **When not:** May not generalize outside English or training domain.\n",
    "* **LLM use:** **Yes (BLEURT pretrained LM)**.\n",
    "\n",
    "---\n",
    "\n",
    "### BERTScore\n",
    "\n",
    "Semantic similarity using contextual embeddings.\n",
    "\n",
    "* **Input:** token embeddings of $p$ and $g$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{Prec} = \\frac{1}{|P|}\\sum_{t\\in P}\\max_{s\\in G}\\cos(e_t,e_s)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{Rec} = \\frac{1}{|G|}\\sum_{s\\in G}\\max_{t\\in P}\\cos(e_s,e_t)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{F1}=\\frac{2PR}{P+R}\n",
    "  $$\n",
    "* **When to use:** Summarization, paraphrase tasks.\n",
    "* **When not:** Requires heavy pretrained model; slower.\n",
    "* **LLM use:** **Yes (DeBERTa scorer)**.\n",
    "\n",
    "---\n",
    "\n",
    "### String Distance (Edit-based)\n",
    "\n",
    "Uses token-level distances.\n",
    "\n",
    "* **Input:** tokens of $p$, $g$.\n",
    "* **Formulation:**\n",
    "\n",
    "  * Edit distance: $\\text{Lev}(p,g)$.\n",
    "  * Edit similarity: $1-\\frac{\\text{Lev}(p,g)}{\\max(|p|,|g|)}$.\n",
    "  * Longest common prefix length.\n",
    "* **When to use:** Low-level signal, e.g. code completion or data deduplication.\n",
    "* **When not:** Not semantic; penalizes synonyms.\n",
    "* **LLM use:** **No external LM**.\n",
    "\n",
    "---\n",
    "\n",
    "### Extractiveness\n",
    "\n",
    "Measures how much a summary copies from the source.\n",
    "\n",
    "* **Input:** source text $s$, summary $p$.\n",
    "* **Formulation:**\n",
    "\n",
    "  * Coverage = fraction of summary tokens in copied spans.\n",
    "  * Density = average length of copied spans.\n",
    "  * Compression = $|s|/|p|$.\n",
    "* **When to use:** Summarization quality assessment.\n",
    "* **When not:** Irrelevant outside summarization.\n",
    "* **LLM use:** **No external LM**.\n",
    "\n",
    "---\n",
    "\n",
    "### Faithfulness (SummaCZS)\n",
    "\n",
    "Zero-shot factual consistency.\n",
    "\n",
    "* **Input:** source $s$, summary $p$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{Faithfulness}(p,s) = f_{\\text{SummaCZS}}(s,p)\\in[0,1]\n",
    "  $$\n",
    "* **When to use:** Summarization or factual generation.\n",
    "* **When not:** Expensive; depends on pretrained model.\n",
    "* **LLM use:** **Yes (SummaCZS model)**.\n",
    "\n",
    "---\n",
    "\n",
    "# Log-probability based metrics\n",
    "\n",
    "These require the evaluated LM’s logits or log-probs over choices.\n",
    "\n",
    "---\n",
    "\n",
    "### Loglikelihood Accuracy\n",
    "\n",
    "Checks if top choice by log-prob is correct.\n",
    "\n",
    "* **Input:** log-probs $\\ell_i$ for each choice $c_i$, gold indices $\\mathcal{G}$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{LLAcc} = \\mathbf{1}\\!\\left[\\arg\\max_i \\ell_i \\in \\mathcal{G}\\right]\n",
    "  $$\n",
    "* **When to use:** MCQ benchmarks, multiple-choice reasoning.\n",
    "* **When not:** Free-form generation.\n",
    "* **LLM use:** **Uses evaluated LM’s logits only**.\n",
    "\n",
    "---\n",
    "\n",
    "### Normalized Multi-Choice Probability\n",
    "\n",
    "Gold probability normalized by total.\n",
    "\n",
    "* **Input:** log-probs $\\ell_i$, gold indices.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  p_i = \\exp(\\ell_i),\\quad\n",
    "  \\text{Score} = \\max_{g\\in\\mathcal{G}} \\frac{p_g}{\\sum_j p_j}\n",
    "  $$\n",
    "* **When to use:** Probability-calibrated multiple-choice tasks.\n",
    "* **When not:** Generation tasks.\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "### Probability\n",
    "\n",
    "Unnormalized gold probability.\n",
    "\n",
    "* **Input:** log-probs $\\ell_i$, gold set.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{Score} = \\max_{g\\in\\mathcal{G}} \\exp(\\ell_g)\n",
    "  $$\n",
    "* **When to use:** Compare model’s absolute confidence.\n",
    "* **When not:** Cross-model comparisons (diff. calibration).\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "### Recall\\@k\n",
    "\n",
    "Does top-k set contain a gold?\n",
    "\n",
    "* **Input:** log-probs $\\ell_i$, gold $\\mathcal{G}$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{Recall@}k = \\mathbf{1}\\!\\left[\\mathcal{G}\\cap \\text{TopK}(\\ell,k)\\neq\\emptyset\\right]\n",
    "  $$\n",
    "* **When to use:** Retrieval-style MCQ tasks.\n",
    "* **When not:** Single-gold free-form tasks.\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "### MRR (Mean Reciprocal Rank)\n",
    "\n",
    "Ranks gold among choices.\n",
    "\n",
    "* **Input:** log-probs $\\ell_i$, gold $\\mathcal{G}$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  r = \\min_{g\\in\\mathcal{G}}\\text{rank}(g),\\quad \\text{MRR} = \\frac{1}{r+1}\n",
    "  $$\n",
    "* **When to use:** Retrieval or MCQ tasks.\n",
    "* **When not:** Generation.\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "### AccGoldLikelihood\n",
    "\n",
    "Checks token-level argmax vs. gold.\n",
    "\n",
    "* **Input:** token logits, gold token sequence.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{Score} = \\mathbf{1}[\\exists\\ \\text{gold target with argmax(logits)=gold}]\n",
    "  $$\n",
    "* **When to use:** Strict token-level evaluation (e.g. code generation correctness).\n",
    "* **When not:** Paraphrastic text.\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "# Sample-based metrics\n",
    "\n",
    "These assume multiple samples $p^{(1)},\\dots,p^{(n)}$ from the same prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### Avg\\@k\n",
    "\n",
    "Average correctness across first $k$ samples.\n",
    "\n",
    "* **Input:** scores $s_1,\\dots,s_k$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{Avg@}k = \\frac{1}{k}\\sum_{i=1}^k s_i\n",
    "  $$\n",
    "* **When to use:** Stability analysis, few-sample correctness.\n",
    "* **When not:** If only one sample per prompt.\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "### Maj\\@k\n",
    "\n",
    "Majority-vote among samples.\n",
    "\n",
    "* **Input:** normalized predictions $\\nu(p^{(i)})$, gold $g$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\hat{y} = \\arg\\max_y \\text{count}\\{\\nu(p^{(i)})=y\\},\\quad\n",
    "  \\text{Maj@}k = s(\\hat{y},g)\n",
    "  $$\n",
    "* **When to use:** Multiple generations per query (MCQ/short answers).\n",
    "* **When not:** Free-form summarization.\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "### Pass\\@k\n",
    "\n",
    "Probability that at least one of $k$ samples solves the task (common in code eval).\n",
    "\n",
    "* **Input:** $n$ samples, $c$ correct ones.\n",
    "* **Formulation (Chen et al., 2021):**\n",
    "\n",
    "  $$\n",
    "  \\text{Pass@}k = \n",
    "  \\begin{cases}\n",
    "  1 & n-c < k\\\\\n",
    "  1 - \\prod_{j=n-c+1}^n \\left(1 - \\frac{k}{j}\\right) & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "* **When to use:** Code generation, where multiple tries are allowed.\n",
    "* **When not:** Open-ended natural language generation.\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "### G-Pass\\@k\n",
    "\n",
    "Generalizes Pass\\@k with success threshold.\n",
    "\n",
    "* **Input:** successes $c$, total $n$, draw size $k$, threshold $t$.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  X\\sim \\text{Hypergeom}(N=n,K=c,n=k),\\quad\n",
    "  \\text{G-Pass@}k(t) = \\Pr[X\\ge \\max(\\lceil kt\\rceil,1)]\n",
    "  $$\n",
    "* **When to use:** Stricter success definitions, multi-test settings.\n",
    "* **When not:** Simple code-eval tasks.\n",
    "* **LLM use:** **Evaluated LM only**.\n",
    "\n",
    "---\n",
    "\n",
    "### JudgeLLM (SimpleQA / MTBench / MixEval)\n",
    "\n",
    "Uses a separate LLM to grade predictions.\n",
    "\n",
    "* **Input:** question, prediction, (optional) gold/reference.\n",
    "* **Formulation:**\n",
    "\n",
    "  $$\n",
    "  \\text{Score} = f_{\\text{judge-LLM}}(\\text{question}, p, g)\n",
    "  $$\n",
    "* **When to use:** Open-ended generation (e.g. chat, reasoning) where automated metrics fail.\n",
    "* **When not:** When budget constraints prevent calling another LLM.\n",
    "* **LLM use:** **Yes — requires external judge LLM (OpenAI, LiteLLM, vLLM, etc.)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ba329",
   "metadata": {},
   "source": [
    "## Where these metrics are used?\n",
    "\n",
    "Got it. Here’s a consolidated table mapping **tasks → common metrics → canonical benchmarks**. I’ll cover the most frequent categories in NLP evaluations.\n",
    "\n",
    "---\n",
    "\n",
    "# NLP Tasks, Metrics, and Benchmarks\n",
    "\n",
    "| Task                                                        | Common Metrics                                                                             | Example Benchmarks                                    |\n",
    "| ----------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ----------------------------------------------------- |\n",
    "| **Question Answering (extractive / factual)**               | Exact Match (EM), F1 (bag-of-words), sometimes ROUGE                                       | SQuAD, NaturalQuestions, TriviaQA, TyDiQA             |\n",
    "| **Open-domain QA (generative)**                             | BLEU, ROUGE-L, BERTScore, LLM-as-judge                                                     | MS MARCO, ELI5, TruthfulQA                            |\n",
    "| **Reading Comprehension / Multiple-Choice QA**              | Loglikelihood Accuracy, Recall\\@k, MRR, Normalized MC Probability                          | RACE, MMLU, ARC (Easy/Challenge)                      |\n",
    "| **Summarization**                                           | ROUGE-1/2/L/Lsum, BERTScore, BLEURT, Extractiveness, Faithfulness (SummaCZS), LLM-as-judge | CNN/DailyMail, XSum, SAMSum, GovReport                |\n",
    "| **Machine Translation**                                     | BLEU, chrF, COMET, BERTScore, BLEURT                                                       | WMT (e.g., WMT14 En-De, WMT21 tasks)                  |\n",
    "| **Paraphrase Identification / Semantic Textual Similarity** | Pearson/Spearman corr. (STS score), BERTScore, cosine similarity                           | GLUE STS-B, MRPC, QQP                                 |\n",
    "| **Text Classification**                                     | Accuracy, Precision, Recall, F1 (macro/micro)                                              | GLUE (SST-2, MNLI, QNLI), AG News, Yelp Reviews       |\n",
    "| **Dialogue / Conversational Agents**                        | BLEU (historical), Distinct-n (diversity), ROUGE, LLM-as-judge                             | DailyDialog, MultiWOZ, MTBench                        |\n",
    "| **Code Generation**                                         | Pass\\@k, Exact Match, BLEU, CodeBLEU                                                       | HumanEval, MBPP, APPS                                 |\n",
    "| **Reasoning / Math Word Problems**                          | Exact Match (final answer), EM\\@k, LLM-as-judge                                            | GSM8K, AQuA-RAT, MATH, AIME24                         |\n",
    "| **Toxicity / Bias / Safety**                                | Accuracy/F1 on toxic label, AUROC, calibration, safety probes                              | CivilComments, RealToxicityPrompts, BBQ, HolisticBias |\n",
    "| **Information Retrieval / Ranking**                         | Recall\\@k, NDCG, MRR                                                                       | MS MARCO, BEIR, TREC                                  |\n",
    "| **Linguistic Acceptability**                                | Accuracy                                                                                   | CoLA (in GLUE), BLiMP                                 |\n",
    "| **Natural Language Inference (NLI)**                        | Accuracy, Macro-F1                                                                         | MNLI, RTE, ANLI                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23e374",
   "metadata": {},
   "source": [
    "### Using lighteval\n",
    "\n",
    "Lighteval uses `Doc`([API Docs](https://huggingface.co/docs/lighteval/en/package_reference/doc)) and `ModelResponse` ([API Docs](https://huggingface.co/docs/lighteval/en/package_reference/models_outputs)) as objects to interact with metrics. \n",
    "We can import various metrics from `lighteval.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064e301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gupta/Downloads/evals/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# examples_lighteval.py\n",
    "from lighteval.tasks.requests import Doc\n",
    "from lighteval.models.model_output import ModelResponse\n",
    "from lighteval.metrics.metrics import F1_score, BLEU, BLEURT, BertScore\n",
    "\n",
    "# or call the metrics from here\n",
    "from lighteval.metrics.metrics import Metrics\n",
    "# Metrics.exact_match --> Uses Enum under the hood: https://github.com/huggingface/lighteval/blob/v0.11.0/src/lighteval/metrics/metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/gupta/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.3333333333333333\n",
      "F1 Score: 0.6666666666666666\n",
      "BLEURT Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "g = [\"A quick brown fox jumps over the lazy dog.\"]\n",
    "p = [\"A fast brown fox leaps over a lazy dog.\"]\n",
    "\n",
    "doc = Doc(\n",
    "    task_name=\"similarity\",\n",
    "    choices=g, \n",
    "    query=\"\", \n",
    "    gold_index=0 # tell the right response index from choices\n",
    ")\n",
    "mr  = ModelResponse(text=p)\n",
    "\n",
    "bleu_score = BLEU(n_gram=2).compute(doc=doc, model_response=mr)\n",
    "f1_score = F1_score().compute(doc=doc, model_response=mr)\n",
    "bleurt_score = BLEURT().compute(doc=doc, model_response=mr)\n",
    "\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"BLEURT Score:\", f1_score)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
